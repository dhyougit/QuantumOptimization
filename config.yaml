# Knowledge-Enhanced Transformer Configuration

# Model Architecture
model:
  vocab_size: 50000
  d_model: 512
  n_heads: 8
  n_layers: 6
  d_ff: 2048
  n_knowledge_heads: 4  # Number of heads for knowledge attention
  max_len: 5000
  dropout: 0.1

# Knowledge Base
knowledge:
  knowledge_dim: 512
  retrieval_method: "dense"  # Options: "dense", "sparse", "hybrid"
  top_k: 10  # Number of knowledge items to retrieve
  use_sparse_attention: false
  knowledge_weight: 0.5  # Weight for knowledge loss component

# Training
training:
  batch_size: 32
  epochs: 10
  learning_rate: 0.0001
  warmup_steps: 4000
  gradient_clip: 1.0
  label_smoothing: 0.1
  val_split: 0.1
  seed: 42
  
  # Optimizer
  optimizer:
    type: "adamw"
    betas: [0.9, 0.98]
    eps: 1e-9
    weight_decay: 0.01
  
  # Scheduler
  scheduler:
    type: "warmup_linear"  # Options: "warmup_linear", "cosine", "constant"
    warmup_ratio: 0.1

# Data
data:
  data_path: "data/training_data.jsonl"
  knowledge_path: "data/knowledge.json"
  max_length: 512
  num_workers: 4

# Evaluation
evaluation:
  eval_batch_size: 64
  eval_steps: 500  # Evaluate every N steps
  metrics:
    - "perplexity"
    - "accuracy"
    - "top5_accuracy"
    - "bleu"

# Logging and Checkpointing
logging:
  use_wandb: false
  project_name: "knowledge-enhanced-transformer"
  log_interval: 100
  save_dir: "checkpoints"
  save_total_limit: 3  # Keep only N best checkpoints

# Generation
generation:
  max_length: 100
  temperature: 1.0
  top_k: 50
  top_p: 0.9
  repetition_penalty: 1.0
  use_knowledge: true

# Hardware
hardware:
  device: "cuda"  # Options: "cuda", "cpu", "mps"
  mixed_precision: true  # Use automatic mixed precision
  num_gpus: 1
